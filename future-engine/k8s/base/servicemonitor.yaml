---
# ServiceMonitor for Prometheus Operator
# Automatically discovers and scrapes metrics from matching engine pods
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: matching-engine
  namespace: matching-engine
  labels:
    app.kubernetes.io/name: matching-engine
    release: prometheus  # Match your Prometheus Operator release
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: matching-engine
  namespaceSelector:
    matchNames:
      - matching-engine
  endpoints:
    - port: metrics
      interval: 15s
      scrapeTimeout: 10s
      path: /metrics
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]
          targetLabel: shard
        - sourceLabels: [__meta_kubernetes_pod_name]
          targetLabel: pod
        - sourceLabels: [__meta_kubernetes_namespace]
          targetLabel: namespace

---
# PrometheusRule for alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: matching-engine-alerts
  namespace: matching-engine
  labels:
    app.kubernetes.io/name: matching-engine
    release: prometheus
spec:
  groups:
    - name: matching-engine.rules
      rules:
        # Alert if a shard has no healthy pods
        - alert: MatchingEngineShardDown
          expr: |
            sum by (shard) (up{job="matching-engine"}) == 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Matching engine shard {{ $labels.shard }} is down"
            description: "All pods for shard {{ $labels.shard }} are unavailable for more than 1 minute."

        # Alert if only standby is running (primary down)
        - alert: MatchingEnginePrimaryDown
          expr: |
            sum by (shard) (up{job="matching-engine", role="primary"}) == 0
            and sum by (shard) (up{job="matching-engine", role="standby"}) > 0
          for: 30s
          labels:
            severity: critical
          annotations:
            summary: "Primary for shard {{ $labels.shard }} is down"
            description: "Primary pod for shard {{ $labels.shard }} is unavailable. Standby should take over."

        # Alert on high memory usage
        - alert: MatchingEngineHighMemory
          expr: |
            (jvm_memory_used_bytes{area="heap"} / jvm_memory_max_bytes{area="heap"}) > 0.9
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage on {{ $labels.pod }}"
            description: "JVM heap usage is above 90% for more than 5 minutes."

        # Alert on high order processing latency
        - alert: MatchingEngineHighLatency
          expr: |
            histogram_quantile(0.99, rate(matching_engine_order_processing_seconds_bucket[5m])) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High latency on shard {{ $labels.shard }}"
            description: "P99 order processing latency is above 100ms for more than 5 minutes."

        # Alert on pod restarts
        - alert: MatchingEnginePodRestarting
          expr: |
            increase(kube_pod_container_status_restarts_total{namespace="matching-engine"}[1h]) > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.pod }} is restarting frequently"
            description: "Pod has restarted more than 3 times in the last hour."
